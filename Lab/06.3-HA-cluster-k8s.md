# Cấu hình HA cluster k8s
**Mô hình LAB**

|Server|IP|Vai trò|etcd|HAproxy|
|--------|------------|------|-----|--------|
|server01|172.16.69.91|master|etcd1|HAproxy1|
|server02|172.16.69.92|master|etcd2|HAproxy2|
|server03|172.16.69.93|master|etcd3|||
|client01|172.16.68.101|worker|||
|client02|172.16.68.102|worker|||
|client03|172.16.68.103|worker|||

***Virtual IP: 172.16.68.100***
## 1. Cài đặt, cấu hình HAproxy+keepalived

Thực hiện trên 2 Node: 172.16.68.91, 172.16.68.92

**Cấu hình gắn IP lên carđ mạng:**
```sh
vim /etc/sysctl.conf
net.ipv4.ip_nonlocal_bind=1
sysctl -p
```
**Install Haproxy, keepalived:**
```sh
yum install haproxy keepalived -y
```
**Setup keepalived configuration:**
```sh
mkdir -p /etc/keepalived
cat << EOF > /etc/keepalived/keepalived.conf
vrrp_script check_haproxy {
script "killall -0 haproxy"
interval 3
}

vrrp_instance LAN_133 {
interface eth0
virtual_router_id 100
priority 100			# Trên Node còn lại là 101 để thực hiện Active-Backup
advert_int 2
nopreempt			# Giữ nguyên vai trò

authentication {
  auth_type AH			# Giống nhau trên các Node
  auth_pass aabbccdd		# Giống nhau trên các Node
}

track_script {
  check_haproxy
}

virtual_ipaddress {
  172.16.68.100/24		# IP HAproxy
  }
}

EOF
```
**Setup Haproxy**
```sh
cat << EOF > /etc/haproxy/haproxy.cfg
global
	log /dev/log	local0
	log /dev/log	local1 notice
	chroot /var/lib/haproxy
	stats timeout 30s
	user haproxy
	group haproxy
	daemon

	# Default SSL material locations
	ca-base /etc/ssl/certs
	crt-base /etc/ssl/private

	# Default ciphers to use on SSL-enabled listening sockets.
	# For more information, see ciphers(1SSL). This list is from:
	#  https://hynek.me/articles/hardening-your-web-servers-ssl-ciphers/
	ssl-default-bind-ciphers ECDH+AESGCM:DH+AESGCM:ECDH+AES256:DH+AES256:ECDH+AES128:DH+AES:ECDH+3DES:DH+3DES:RSA+AESGCM:RSA+AES:RSA+3DES:!aNULL:!MD5:!DSS
	ssl-default-bind-options no-sslv3

defaults
	log	global
	mode	http
	option	httplog
	option	dontlognull
        timeout connect 5000
        timeout client  50000
        timeout server  50000

frontend k8s
bind 172.16.68.100:6445 		# IP HAProxy
option tcplog
mode tcp
default_backend k8s-master-nodes

backend k8s-master-nodes
mode tcp
balance roundrobin
option tcp-check
server server01 172.16.68.91:6443 check fall 3 rise 2
server server02 172.16.68.92:6443 check fall 3 rise 2
server server03 172.16.68.93:6443 check fall 3 rise 2

EOF
```
**Thực hiện restart haproxy và keepalived:**
```sh
systemctl restart haproxy
systemctl restart keepalived
```
**Kiểm tra:**
```sh
[root@server01 ~]# ip a | grep eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    inet 172.16.68.91/24 brd 172.16.68.255 scope global noprefixroute eth0
    inet 172.16.68.100/24 scope global secondary eth0

$ systemctl status haproxy
$ systemctl status keepalived
```
## 2. Cài đặt, cấu hình HA Etcd cluster:

### 2.1. Install cfssl (Cloudflare ssl) trên Master 1:
```sh
wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64
wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
chmod +x cfssl*
mv cfssl_linux-amd64 /usr/local/bin/cfssl
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson
```
Xác thực cài đặt
```sh
[root@server01 ~]# cfssl version
Version: 1.2.0
Revision: dev
Runtime: go1.6
```
### 2.2 Tạo TLS certificates:
**Tạo file cấu hình CA (certificate authority):**
```sh
cat << EOF > ca-config.json
{
  "signing": {
    "default": {
      "expiry": "8760h"
    },
    "profiles": {
      "kubernetes": {
        "usages": ["signing", "key encipherment", "server auth", "client auth"],
        "expiry": "8760h"
      }
    }
  }
}
EOF
```
**Tạo certificate authority signing request configuration file.**
```sh
cat << EOF > ca-csr.json
{
  "CN": "Kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
  {
    "C": "IE",
    "L": "VNPT",
    "O": "Kubernetes",
    "OU": "CA",
    "ST": "VNPT Co."
  }
 ]
}
EOF
```
**Tạo certificate authority certificate và private key**
```sh
[root@server01 ~]# cfssl gencert -initca ca-csr.json | cfssljson -bare ca
```
## Tạo certificate cho Etcd cluster
**Tạo certificate signing request configuration file**
```sh
cat << EOF > kubernetes-csr.json
{
  "CN": "kubernetes",
  "key": {
    "algo": "rsa",
    "size": 2048
  },
  "names": [
  {
    "C": "IE",
    "L": "VNPT",
    "O": "Kubernetes",
    "OU": "Kubernetes",
    "ST": "VNPT Co."
  }
 ]
}
EOF
```
**Generate the certificate and private key**
```sh
[root@server01 ~]# cfssl gencert \
-ca=ca.pem \
-ca-key=ca-key.pem \
-config=ca-config.json \
-hostname=172.16.68.91,172.16.68.92,172.16.68.93,172.16.68.100 \
-profile=kubernetes kubernetes-csr.json | \
cfssljson -bare kubernetes
```
**Copy certificate tới mỗi Node trong etcd cluster**
```sh
scp ca.pem kubernetes.pem kubernetes-key.pem root@172.16.68.92:~
scp ca.pem kubernetes.pem kubernetes-key.pem root@172.16.68.93:~
```
## 3. Install và config Etcd cluster
### 3.1 Thực hiện trên tất cả các Node
Tạo thư mục configuration cho `etcd`
```sh
mkdir /etc/etcd /var/lib/etcd
```
Move certificates tới thư mục configuration
```sh
mv ~/ca.pem ~/kubernetes.pem ~/kubernetes-key.pem /etc/etcd
```
Download etcd binaries và giải nén
```sh
wget https://github.com/coreos/etcd/releases/download/v3.3.9/etcd-v3.3.9-linux-amd64.tar.gz
tar xvzf etcd-v3.3.9-linux-amd64.tar.gz
```
Move etcd binaries tới `/usr/local/bin`
```sh
mv etcd-v3.3.9-linux-amd64/etcd* /usr/local/bin/
```
### 3.2 Thực hiện trên từng Node
**Thực hiện trên Node 1
```sh
cat << EOF> /etc/systemd/system/etcd.service

[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \
  --name etcd1 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://172.16.68.91:2380 \
  --listen-peer-urls https://172.16.68.91:2380 \
  --listen-client-urls https://172.16.68.91:2379 \
  --advertise-client-urls https://172.16.68.91:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster etcd1=https://172.16.68.91:2380,etcd2=https://172.16.68.92:2380,etcd3=https://172.16.68.93:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
**Thực hiện trên Node 2
```sh
cat << EOF > /etc/systemd/system/etcd.service

[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \
  --name etcd2 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://172.16.68.92:2380 \
  --listen-peer-urls https://172.16.68.92:2380 \
  --listen-client-urls https://172.16.68.92:2379 \
  --advertise-client-urls https://172.16.68.92:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster etcd1=https://172.16.68.91:2380,etcd2=https://172.16.68.92:2380,etcd3=https://172.16.68.93:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
**Thực hiện trên Node 3**
```sh
cat << EOF > /etc/systemd/system/etcd.service

[Unit]
Description=etcd
Documentation=https://github.com/coreos

[Service]
Type=notify
ExecStart=/usr/local/bin/etcd \
  --name etcd3 \
  --cert-file=/etc/etcd/kubernetes.pem \
  --key-file=/etc/etcd/kubernetes-key.pem \
  --peer-cert-file=/etc/etcd/kubernetes.pem \
  --peer-key-file=/etc/etcd/kubernetes-key.pem \
  --trusted-ca-file=/etc/etcd/ca.pem \
  --peer-trusted-ca-file=/etc/etcd/ca.pem \
  --peer-client-cert-auth \
  --client-cert-auth \
  --initial-advertise-peer-urls https://172.16.68.93:2380 \
  --listen-peer-urls https://172.16.68.93:2380 \
  --listen-client-urls https://172.16.68.93:2379 \
  --advertise-client-urls https://172.16.68.93:2379 \
  --initial-cluster-token etcd-cluster-0 \
  --initial-cluster etcd1=https://172.16.68.91:2380,etcd2=https://172.16.68.92:2380,etcd3=https://172.16.68.93:2380 \
  --initial-cluster-state new \
  --data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```
### 3.3 Thực hiện trên cả 3 Node
```sh
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
```
**Xác thực cluster**
```sh
[root@server01 ~]# ETCDCTL_API=3 etcdctl --endpoints=https://172.16.68.91:2379,https://172.16.68.92:2379,https://172.16.68.93:2379 --cacert=/etc/etcd/ca.pem --cert=/etc/etcd/kubernetes.pem --key=/etc/etcd/kubernetes-key.pem --write-out=table endpoint status
+---------------------------+------------------+---------+---------+-----------+-----------+------------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | RAFT TERM | RAFT INDEX |
+---------------------------+------------------+---------+---------+-----------+-----------+------------+
| https://172.16.68.91:2379 | cffdcdc445e8a491 |   3.3.9 |   20 kB |      true |         2 |          8 |
| https://172.16.68.92:2379 | 664d530e70f8fcf7 |   3.3.9 |   20 kB |     false |         2 |          8 |
| https://172.16.68.93:2379 | b6fac6f70b73ac21 |   3.3.9 |   20 kB |     false |         2 |          8 |
+---------------------------+------------------+---------+---------+-----------+-----------+------------+
```
## 3. Cài đặt, cấu hình master node k8s và worker node k8s:
### 3.1 Thực hiện trên 3 Node Master và 3 Node Worker

Xem chi tiết cài đặt [tại đây](https://github.com/quangln94/Linux/blob/master/Kubernetes/Lab/00-Install-k8s.md)

Disable swap:
```sh
$ swapoff -a
$ vim /etc/fstab
#/dev/mapper/cl-swap swap swap defaults 0 0
```
```sh
$ cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
$ sysctl --system
```
Tắt firewalld và SElinux
```sh
systemctl stop firewalld
systemctl disable firewalld
sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/config
```
Cài đặt Kubeadm
```sh
$ cat <<EOF > /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
$ yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
$ systemctl enable kubelet
```
Cài đặt Docker
```sh
yum -y install docker
systemctl start docker
systemctl enable docker
```
### 3.2 Thực hiện trên từng Node
**Thực hiện trên Node Master 1:**

***Copy cert to /etc/kubernetes/pki:***
```sh
mkdir -p /etc/kubernetes/pki/etcd
cp /etc/etcd/ca.pem /etc/kubernetes/pki/etcd/
cp /etc/etcd/kubernetes.pem /etc/kubernetes/pki/
cp /etc/etcd/kubernetes-key.pem /etc/kubernetes/pki/
```
Tạo file `kubeadm-config.yaml` trên Node master 1:
```sh
cat << EOF > kubeadm-config.yaml
apiVersion: kubeadm.k8s.io/v1beta1
kind: ClusterConfiguration
kubernetesVersion: stable
controlPlaneEndpoint: "172.16.68.100:6445"
etcd:
    external:
        endpoints:
        - https://172.16.68.91:2379
        - https://172.16.68.92:2379
        - https://172.16.68.93:2379
	caFile: /etc/kubernetes/pki/etcd/ca.pem
        certFile: /etc/kubernetes/pki/kubernetes.pem
        keyFile: /etc/kubernetes/pki/kubernetes-key.pem
networking:
    podSubnet: "10.244.0.0/16"
EOF
```
***Thực hiện khởi tạo node master đầu tiên trong cụm k8s:***
```sh
kubeadm init --config=kubeadm-config.yaml --upload-certs
```
Trong output ở trên có các dòng sau, sử dụng để join các node master và các node worker vào cụm cluster ở phần dưới:
```sh
You can now join any number of the control-plane node running the following command on each as root:

kubeadm join 172.16.68.100:6445 --token 0rhev0.mrunhgzi07zb9cao \
  --discovery-token-ca-cert-hash sha256:923716564e6113f5c83c559ed345e766c81119dda427769e5df151a018dd42ed \
  --control-plane --certificate-key 845be50a981f3bb69198d52203c77db87f6d01062bfe635fc4d7bd1d7ae3262e
------------------------------------------------------------------------------------------------------------
Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.16.68.100:6445 --token 0rhev0.mrunhgzi07zb9cao \
    --discovery-token-ca-cert-hash sha256:923716564e6113f5c83c559ed345e766c81119dda427769e5df151a018dd42ed
```
***Khởi tạo môi trường:***
```sh
mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```
***Configure Pod Network with Flannel***
```sh
kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
```
***Join các node master vào cụm k8s, thực hiện trên 2 node master2 và master3:***
```sh
kubeadm join 172.16.68.100:6445 --token 0rhev0.mrunhgzi07zb9cao \
  --discovery-token-ca-cert-hash sha256:923716564e6113f5c83c559ed345e766c81119dda427769e5df151a018dd42ed \
  --control-plane --certificate-key 845be50a981f3bb69198d52203c77db87f6d01062bfe635fc4d7bd1d7ae3262e

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config
```
***Để thực hiện 1 node vừa làm master và worker, ta cần thực hiện lệnh sau:***
```sh
kubectl taint nodes --all node-role.kubernetes.io/master-
```
***Join các node worker vào cụm k8s, thực hiện trên 3 node worker:***
```sh
kubeadm join 172.16.68.100:6445 --token 0rhev0.mrunhgzi07zb9cao \
    --discovery-token-ca-cert-hash sha256:923716564e6113f5c83c559ed345e766c81119dda427769e5df151a018dd42ed
```
Kiểm tra Node trong cụm k8s:
```sh
[root@server01 ~]# kubectl get node -o wide
NAME       STATUS   ROLES    VERSION   INTERNAL-IP     OS-IMAGE                KERNEL-VERSION               CONTAINER-RUNTIME
client01   Ready    <none>   v1.16.3   172.16.68.101   CentOS Linux 7 (Core)   3.10.0-1062.4.3.el7.x86_64   docker://1.13.1
client02   Ready    <none>   v1.16.3   172.16.68.102   CentOS Linux 7 (Core)   3.10.0-1062.4.3.el7.x86_64   docker://1.13.1
client03   Ready    <none>   v1.16.3   172.16.68.103   CentOS Linux 7 (Core)   3.10.0-1062.4.3.el7.x86_64   docker://1.13.1
server01   Ready    master   v1.16.3   172.16.68.91    CentOS Linux 7 (Core)   3.10.0-1062.4.3.el7.x86_64   docker://1.13.1
server02   Ready    master   v1.16.3   172.16.68.92    CentOS Linux 7 (Core)   3.10.0-1062.4.3.el7.x86_64   docker://1.13.1
server03   Ready    master   v1.16.3   172.16.68.93    CentOS Linux 7 (Core)   3.10.0-1062.4.3.el7.x86_64   docker://1.13.1

```
***Check các pod trong namespace kube-system:***
```sh
[root@server01 ~]# kubectl get pods -o wide -A
NAMESPACE     NAME                               READY   STATUS    RESTARTS   AGE     IP              NODE       NOMINATED	NODE   READINESS GATES
kube-system   coredns-5644d7b6d9-lwlqg           1/1     Running   0          13m     10.244.0.2      server01   <none>         <none>
kube-system   coredns-5644d7b6d9-wj294           1/1     Running   0          13m     10.244.0.3      server01   <none>         <none>
kube-system   kube-apiserver-server01            1/1     Running   0          13m     172.16.68.91    server01   <none>         <none>
kube-system   kube-apiserver-server02            1/1     Running   0          11m     172.16.68.92    server02   <none>         <none>
kube-system   kube-apiserver-server03            1/1     Running   0          11m     172.16.68.93    server03   <none>         <none>
kube-system   kube-controller-manager-server01   1/1     Running   0          12m     172.16.68.91    server01   <none>         <none>
kube-system   kube-controller-manager-server02   1/1     Running   0          11m     172.16.68.92    server02   <none>         <none>
kube-system   kube-controller-manager-server03   1/1     Running   0          11m     172.16.68.93    server03   <none>         <none>
kube-system   kube-flannel-ds-amd64-2hww2        1/1     Running   0          11m     172.16.68.92    server02   <none>         <none>
kube-system   kube-flannel-ds-amd64-99ksk        1/1     Running   1          8m47s   172.16.68.103   client03   <none>         <none>
kube-system   kube-flannel-ds-amd64-9gt2j        1/1     Running   0          11m     172.16.68.93    server03   <none>         <none>
kube-system   kube-flannel-ds-amd64-dwpc7        1/1     Running   1          8m47s   172.16.68.101   client01   <none>         <none>
kube-system   kube-flannel-ds-amd64-jmh7g        1/1     Running   0          8m47s   172.16.68.102   client02   <none>         <none>
kube-system   kube-flannel-ds-amd64-m6mzp        1/1     Running   0          12m     172.16.68.91    server01   <none>         <none>
kube-system   kube-proxy-5s5dq                   1/1     Running   0          13m     172.16.68.91    server01   <none>         <none>
kube-system   kube-proxy-7z89z                   1/1     Running   0          11m     172.16.68.93    server03   <none>         <none>
kube-system   kube-proxy-dsj6v                   1/1     Running   0          8m47s   172.16.68.102   client02   <none>         <none>
kube-system   kube-proxy-lpkmn                   1/1     Running   0          8m47s   172.16.68.103   client03   <none>         <none>
kube-system   kube-proxy-sdqdr                   1/1     Running   0          11m     172.16.68.92    server02   <none>         <none>
kube-system   kube-proxy-w4zbb                   1/1     Running   0          8m47s   172.16.68.101   client01   <none>         <none>
kube-system   kube-scheduler-server01            1/1     Running   1          12m     172.16.68.91    server01   <none>         <none>
kube-system   kube-scheduler-server02            1/1     Running   0          11m     172.16.68.92    server02   <none>         <none>
kube-system   kube-scheduler-server03            1/1     Running   0          11m     172.16.68.93    server03   <none>         <none>
```
***Chú ý: Mặc định, trong cụm k8s khi 1 node worker down thì thời gian các pods trên node đó tự động được tạo lại trên node worker khác là 5'. Ta có thể giảm thời gian này xuống, để các pods được tạo lại trên node worker khác nhanh hơn (ở đây tôi set khoảng thời gian này = 1') bằng cách thực hiện các bước sau:***

- ***Bước 1: Thêm `--pod-eviction-timeout=30s` vào file `/etc/kubernetes/manifests/kube-controller-manager.yaml`***
```sh
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-controller-manager
    - --allocate-node-cidrs=true
    - --bind-address=127.0.0.1
    - --pod-eviction-timeout=30s
	...
```  
- ***Bước 2: Thêm `default-not-ready-toleration-seconds=30` và `default-unreachable-toleration-seconds=30` vào file `/etc/kubernetes/manifests/kube-apiserver.yaml` (thực hiện đối với k8s từ phiên bản từ 1.11 trở lên)***
```sh
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=172.168.68.210
    - --default-not-ready-toleration-seconds=30
    - --default-unreachable-toleration-seconds=30
    - --allow-privileged=true
    ...
```
## Tài liệu hướng dẫn
- https://github.com/thangtq710/Kubernetes/blob/master/docs/3.Install_HA_cluster_K8s.md
- https://medium.com/faun/configuring-ha-kubernetes-cluster-on-bare-metal-servers-with-kubeadm-1-2-1e79f0f7857b
